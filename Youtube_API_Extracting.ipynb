{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # For file and directory operations\n",
    "import time  # For adding delays to avoid hitting API rate limits\n",
    "import requests  # For downloading thumbnails\n",
    "import pandas as pd  # For creating and saving dataframes\n",
    "from googleapiclient.discovery import build  # For interacting with the YouTube Data API\n",
    "\n",
    "# Initialize YouTube API\n",
    "api_key = \"AIzaSyCmT0GwAIlQxEMqisj-Jsm208Q6XRX_k9w\"\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# 1. Fetch 100 gaming videos with pagination\n",
    "video_ids = []\n",
    "next_page_token = None\n",
    "total_to_fetch = 500\n",
    "\n",
    "while len(video_ids) < total_to_fetch:\n",
    "    remaining = total_to_fetch - len(video_ids)\n",
    "    search_response = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        type=\"video\",\n",
    "        videoCategoryId=\"20\",  # Gaming category ID\n",
    "        maxResults=min(50, remaining),  # API max is 50 per request\n",
    "        pageToken=next_page_token\n",
    "    ).execute()\n",
    "    \n",
    "    video_ids += [item['id']['videoId'] for item in search_response['items']]\n",
    "    next_page_token = search_response.get('nextPageToken')\n",
    "    \n",
    "    if not next_page_token or len(video_ids) >= total_to_fetch:\n",
    "        break\n",
    "\n",
    "print(f\"Retrieved {len(video_ids)} video IDs.\")\n",
    "\n",
    "# 2. Download thumbnails\n",
    "thumbnail_dir = r\"C:\\Users\\ChanK\\OneDrive - Tilburg University\\Thesis 2024\\YT Thumbnails\"\n",
    "os.makedirs(thumbnail_dir, exist_ok=True)\n",
    "\n",
    "for vid in video_ids:\n",
    "    try:\n",
    "        response = requests.get(f\"https://img.youtube.com/vi/{vid}/maxresdefault.jpg\")\n",
    "        if response.status_code == 200:\n",
    "            with open(os.path.join(thumbnail_dir, f\"{vid}.jpg\"), \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "        else:\n",
    "            print(f\"Thumbnail not available for video ID: {vid}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading thumbnail for {vid}: {str(e)}\")\n",
    "\n",
    "print(\"Thumbnails downloaded successfully.\")\n",
    "\n",
    "# 3. Fetch metadata for each video\n",
    "metadata = []\n",
    "for vid in video_ids:\n",
    "    try:\n",
    "        response = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics,status\",\n",
    "            id=vid\n",
    "        ).execute()\n",
    "        \n",
    "        for item in response['items']:\n",
    "            metadata.append({\n",
    "                \"video_id\": vid,\n",
    "                \"title\": item['snippet']['title'],\n",
    "                \"description\": item['snippet']['description'],\n",
    "                \"published_at\": item['snippet']['publishedAt'],\n",
    "                \"duration\": item['contentDetails']['duration'],\n",
    "                \"definition\": item['contentDetails']['definition'],\n",
    "                \"dimension\": item['contentDetails']['dimension'],\n",
    "                \"caption\": item['contentDetails']['caption'],\n",
    "                \"licensedContent\": item['contentDetails']['licensedContent'],\n",
    "                \"view_count\": int(item['statistics'].get('viewCount', 0)),\n",
    "                \"like_count\": int(item['statistics'].get('likeCount', 0)),\n",
    "                \"comment_count\": int(item['statistics'].get('commentCount', 0)),\n",
    "                \"privacy_status\": item['status']['privacyStatus']\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching metadata for video ID {vid}: {str(e)}\")\n",
    "\n",
    "metadata_df = pd.DataFrame(metadata)\n",
    "print(\"Metadata fetched successfully.\")\n",
    "\n",
    "# 4. Fetch comments for each video with pagination\n",
    "comments = []\n",
    "for vid in video_ids:\n",
    "    next_comment_page = None\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            response = youtube.commentThreads().list(\n",
    "                part=\"snippet,replies\",\n",
    "                videoId=vid,\n",
    "                maxResults=100,  # Maximum allowed per request\n",
    "                pageToken=next_comment_page,\n",
    "                textFormat=\"plainText\"\n",
    "            ).execute()\n",
    "            \n",
    "            for item in response['items']:\n",
    "                top_comment = item['snippet']['topLevelComment']['snippet']\n",
    "                \n",
    "                # Gather replies if available\n",
    "                replies = [\n",
    "                    {\n",
    "                        \"text\": reply['snippet']['textDisplay'],\n",
    "                        \"author\": reply['snippet']['authorDisplayName'],\n",
    "                        \"timestamp\": reply['snippet']['publishedAt'],\n",
    "                        \"likes\": reply['snippet']['likeCount']\n",
    "                    } for reply in item.get('replies', {}).get('comments', [])\n",
    "                ]\n",
    "                \n",
    "                comments.append({\n",
    "                    \"video_id\": vid,\n",
    "                    \"comment\": top_comment['textDisplay'],\n",
    "                    \"author\": top_comment['authorDisplayName'],\n",
    "                    \"timestamp\": top_comment['publishedAt'],\n",
    "                    \"likes\": top_comment['likeCount'],\n",
    "                    \"replies\": replies\n",
    "                })\n",
    "            \n",
    "            next_comment_page = response.get('nextPageToken')\n",
    "            if not next_comment_page:  # Exit if no more pages\n",
    "                break\n",
    "            \n",
    "            time.sleep(1)  # Add delay to avoid hitting quota limits\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching comments for video ID {vid}: {str(e)}\")\n",
    "            break\n",
    "\n",
    "comments_df = pd.DataFrame(comments)\n",
    "print(\"Comments fetched successfully.\")\n",
    "\n",
    "# Save metadata and comments to CSV files (optional)\n",
    "metadata_df.to_csv(r\"C:\\Users\\ChanK\\OneDrive - Tilburg University\\Thesis 2024\\metadata.csv\", index=False)\n",
    "comments_df.to_csv(r\"C:\\Users\\ChanK\\OneDrive - Tilburg University\\Thesis 2024\\comments.csv\", index=False)\n",
    "\n",
    "print(\"Data saved to CSV files.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
