{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Verify directory exists\n",
    "THUMBNAILS_DIR = r\"C:\\Users\\ChanK\\OneDrive - Tilburg University\\Thesis 2024\\YT Thumbnails\"\n",
    "if not os.path.exists(THUMBNAILS_DIR):\n",
    "    raise FileNotFoundError(f\"Thumbnail directory not found: {THUMBNAILS_DIR}\")\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Ensure all required packages are installed.\"\"\"\n",
    "    required_packages = [\n",
    "        \"deepface\", \n",
    "        \"opencv-python\", \n",
    "        \"torch\", \n",
    "        \"transformers\", \n",
    "        \"umap-learn\", \n",
    "        \"hdbscan\", \n",
    "        \"colorthief\", \n",
    "        \"easyocr\", \n",
    "        \"vaderSentiment\", \n",
    "        \"ultralytics\"\n",
    "    ]\n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], check=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Failed to install {package}: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "# Install all required packages before running the rest of the code\n",
    "install_packages()\n",
    "\n",
    "def get_thumbnails(num_samples=None):\n",
    "    \"\"\"Load first N thumbnails from directory.\"\"\"\n",
    "    files = [f for f in os.listdir(THUMBNAILS_DIR) if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "    return [os.path.join(THUMBNAILS_DIR, f) for f in files[:num_samples]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from deepface import DeepFace\n",
    "\n",
    "def analyze_faces_deepface(num_samples=10):\n",
    "    results = {}\n",
    "\n",
    "    for img_path in get_thumbnails(num_samples):\n",
    "        # Load image using OpenCV\n",
    "        img = cv2.imread(img_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB for DeepFace\n",
    "\n",
    "        try:\n",
    "            # Analyze the image for face detection\n",
    "            analysis = DeepFace.analyze(img_path, detector_backend=\"retinaface\", actions=['age', 'gender', 'emotion'])\n",
    "\n",
    "            face_count = len(analysis) if isinstance(analysis, list) else 1\n",
    "            face_data = analysis if isinstance(analysis, list) else [analysis]\n",
    "\n",
    "            # Draw bounding boxes on the image\n",
    "            for face in face_data:\n",
    "                if \"region\" in face:\n",
    "                    x, y, w, h = face[\"region\"][\"x\"], face[\"region\"][\"y\"], face[\"region\"][\"w\"], face[\"region\"][\"h\"]\n",
    "                    cv2.rectangle(img_rgb, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green box\n",
    "\n",
    "            results[img_path] = {\n",
    "                'face_count': face_count,\n",
    "                'face_data': face_data\n",
    "            }\n",
    "\n",
    "            # Display the image with face bounding boxes\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            plt.imshow(img_rgb)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(f\"Detected Faces: {face_count}\")\n",
    "            plt.show()\n",
    "\n",
    "        except Exception as e:\n",
    "            results[img_path] = {'error': str(e)}\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_faces_deepface(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def analyze_composition(num_samples=10):\n",
    "    comp_results = {}\n",
    "    \n",
    "    for img_path in get_thumbnails(num_samples):\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB for correct colors\n",
    "\n",
    "        # Compute saliency heatmap\n",
    "        saliency = cv2.saliency.StaticSaliencyFineGrained_create()\n",
    "        _, sal_map = saliency.computeSaliency(img)\n",
    "\n",
    "        # Convert saliency map to a heatmap\n",
    "        sal_map = (sal_map * 255).astype(np.uint8)  # Scale values to 0-255\n",
    "        sal_map_colored = cv2.applyColorMap(sal_map, cv2.COLORMAP_JET)  # Apply heatmap\n",
    "        sal_map_colored = cv2.cvtColor(sal_map_colored, cv2.COLOR_BGR2RGB)  # Convert for matplotlib\n",
    "\n",
    "        # Rule of thirds grid calculation\n",
    "        height, width = img.shape[:2]\n",
    "        thirds = [(width//3, height//3), (2*width//3, 2*height//3)]\n",
    "\n",
    "        comp_results[img_path] = {\n",
    "            'saliency_heatmap': sal_map.tolist(),\n",
    "            'rule_of_thirds_grid': thirds\n",
    "        }\n",
    "\n",
    "        # Display side-by-side images\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        \n",
    "        # Left: Original Image\n",
    "        axes[0].imshow(img)\n",
    "        axes[0].set_title(\"Original Image\")\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        # Right: Saliency Heatmap\n",
    "        axes[1].imshow(sal_map_colored)\n",
    "        axes[1].set_title(\"Saliency Heatmap\")\n",
    "        axes[1].axis(\"off\")\n",
    "\n",
    "        plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_composition(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from colorthief import ColorThief\n",
    "\n",
    "def analyze_colors(num_samples=10):\n",
    "    color_data = {}\n",
    "    for img_path in get_thumbnails(num_samples):\n",
    "        # Load and display the image\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB for correct colors\n",
    "\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')  # Hide axis\n",
    "        plt.title(f\"Image: {img_path.split('/')[-1]}\")  # Show filename in title\n",
    "        plt.show()\n",
    "\n",
    "        # Perform color analysis\n",
    "        ct = ColorThief(img_path)\n",
    "        color_data[img_path] = {\n",
    "            'dominant': ct.get_color(quality=1),\n",
    "            'palette': ct.get_palette(color_count=5)\n",
    "        }\n",
    "\n",
    "        # Print the analysis\n",
    "        print(f\"  Dominant Color: {color_data[img_path]['dominant']}\")\n",
    "        print(f\"  Color Palette: {color_data[img_path]['palette']}\\n\")\n",
    "\n",
    "    return color_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_colors(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def detect_objects(num_samples=10):\n",
    "    model = YOLO('yolov8n.pt')  # Load YOLOv8 nano model\n",
    "    results = {}\n",
    "\n",
    "    for img_path in get_thumbnails(num_samples):\n",
    "        img = cv2.imread(img_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for display\n",
    "\n",
    "        # Run YOLOv8 object detection\n",
    "        detections = model(img_path)[0]  # Get first result\n",
    "        class_names = detections.names  # Dictionary of class ID to label\n",
    "        \n",
    "        objects_detected = []\n",
    "        boxes = []\n",
    "\n",
    "        for box, cls_id in zip(detections.boxes.xyxy.tolist(), detections.boxes.cls.tolist()):\n",
    "            label = class_names[int(cls_id)]  # Convert class ID to name\n",
    "            objects_detected.append(label)\n",
    "            boxes.append(box)\n",
    "\n",
    "            # Draw bounding box and label on image\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            cv2.rectangle(img_rgb, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green box\n",
    "            cv2.putText(img_rgb, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        results[img_path] = {\n",
    "            'objects': objects_detected,\n",
    "            'boxes': boxes\n",
    "        }\n",
    "\n",
    "        # Show image with bounding boxes\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(img_rgb)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Detected Objects: {', '.join(objects_detected) if objects_detected else 'None'}\")\n",
    "        plt.show()\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_objects(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCR Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from easyocr import Reader\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def analyze_text(num_samples=10):\n",
    "    reader = Reader(['en'])\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    text_results = {}\n",
    "    \n",
    "    for img_path in get_thumbnails(num_samples):\n",
    "        img = cv2.imread(img_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert for matplotlib\n",
    "\n",
    "        detections = reader.readtext(img_path)\n",
    "\n",
    "        extracted_texts = []\n",
    "        sentiment_scores = []\n",
    "        \n",
    "        for box, text, confidence in detections:\n",
    "            extracted_texts.append(text)\n",
    "            sentiment_scores.append({\n",
    "                'text': text,\n",
    "                'score': sia.polarity_scores(text)\n",
    "            })\n",
    "\n",
    "            # Draw bounding boxes around detected text\n",
    "            x_min, y_min = map(int, box[0])  # Top-left corner\n",
    "            x_max, y_max = map(int, box[2])  # Bottom-right corner\n",
    "            cv2.rectangle(img_rgb, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "            cv2.putText(img_rgb, text, (x_min, y_min - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        text_results[img_path] = {\n",
    "            'text': extracted_texts,\n",
    "            'sentiment': sentiment_scores\n",
    "        }\n",
    "\n",
    "        # Display the image with detected text boxes\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(img_rgb)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Detected Text & Sentiment\")\n",
    "        plt.show()\n",
    "    \n",
    "    return text_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_text(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Styles (CLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import umap.umap_ as umap\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.offsetbox as offsetbox\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "def cluster_styles(num_samples=10):\n",
    "    # Load CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    embeddings = []\n",
    "    img_paths = get_thumbnails(num_samples)  # Get image paths\n",
    "\n",
    "    for img_path in img_paths:\n",
    "        image = Image.open(img_path)\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "        # Extract image embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model.get_image_features(**inputs)\n",
    "\n",
    "        embeddings.append(outputs.squeeze(0).detach().cpu().numpy())\n",
    "\n",
    "    # Convert embeddings into NumPy array\n",
    "    embeddings = np.vstack(embeddings)\n",
    "\n",
    "    # Reduce dimensionality\n",
    "    reducer = umap.UMAP(n_neighbors=5, min_dist=0.3, metric='cosine')\n",
    "    reduced_embeds = reducer.fit_transform(embeddings)\n",
    "\n",
    "    # Perform clustering\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=3, metric='euclidean')\n",
    "    cluster_labels = clusterer.fit_predict(reduced_embeds)\n",
    "\n",
    "    # Map images to clusters\n",
    "    cluster_mapping = {img_path: label for img_path, label in zip(img_paths, cluster_labels)}\n",
    "\n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    scatter = ax.scatter(reduced_embeds[:, 0], reduced_embeds[:, 1], c=cluster_labels, cmap='tab10', s=50, edgecolors='k')\n",
    "\n",
    "    # Add images as annotations\n",
    "    for i, (x, y) in enumerate(reduced_embeds):\n",
    "        img = Image.open(img_paths[i])\n",
    "        img.thumbnail((30, 30))  # Resize for visualization\n",
    "        imagebox = offsetbox.AnnotationBbox(offsetbox.OffsetImage(img, zoom=0.5), (x, y), frameon=False)\n",
    "        ax.add_artist(imagebox)\n",
    "\n",
    "    ax.set_title(\"Thumbnail Style Clustering (UMAP + HDBSCAN)\")\n",
    "    plt.colorbar(scatter, label=\"Cluster ID\")\n",
    "    plt.show()\n",
    "\n",
    "    return cluster_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_styles(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
